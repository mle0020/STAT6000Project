---
title: "Stat6000 Project"
output:
  html_document:
    highlight: tango
    theme: spacelab
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries,message=FALSE,warning=FALSE}
library(ggplot2)
library(caret)
library(tidyverse)
library(ISLR)
library(randomForest)
library(class)
library(e1071)
library(MASS)
library(caTools)
```


#Get Data Ready

First we need to read in our data set

```{r data set}
#below is one way to read in the data
#winequality.red <- read.csv("~/GitHub/STAT6000Project/winequality-red.csv", sep=";")

#This is another way to read in the data and the one that 
#we will use for the project
winequality.red<-read.csv("winequality-red.csv",sep = ";")
head(winequality.red)
```

Now we need to make sure there are no empty values or missing values


```{r fix data}
summary(complete.cases(winequality.red))
```

Doing this check shows us that we do not have any missing values in our data 
set.
Now lets do some summary statistics and plots of the response 
value of quality.

```{r data check}
summary(winequality.red)

#bar graph of the data
ggplot(data = winequality.red) + 
  geom_bar(mapping = aes(x = quality))+
  geom_vline(xintercept=mean(winequality.red$quality), color="purple")+
  geom_vline(xintercept=median(winequality.red$quality), color="cyan")+theme_bw()


```

From the data page, it points out that the wine is good if its quality is larger than 6.5, then we create a new factor variable which will be used for our model response, if `quality` is bigger than 6.5, it is equal to 1, otherwise, it is 0.
Moreover, we notice that the number of "bad" red wine is much more than the good ones, and it might influence our experiment.

```{r}
winequality.red$Quality <- rep(0,dim(winequality.red)[1])
winequality.red$Quality[winequality.red$quality>6.5] <- "1"
winequality.red$Quality <- as.factor(winequality.red$Quality)
wine<-winequality.red[,c(1:11,13)] ###dataset for below steps###
head(wine)
table(winequality.red$Quality)
```

Also, we will create a boxplot of each predictor and check the difference between 
good quality wine (1)and bad quality wine (0) for each variable. 
Except for `residual.sugar`, `chlorides`, `free.sulfur.dioxide`, and `PH`, 
there are obvious differences between "good" and "bad" wines. 
In addition, the box-plots show that there are a lot of outliers 
in different predictors, and it could be the reason for misclassification.

```{r}

par(mfrow=c(2,3))
for (v in 1:11) {
  boxplot(winequality.red[,v]~Quality,data = winequality.red,
          ylab = colnames(winequality.red)[v] )
}
```

#Cross Validation for Classification
For classification problems, cross validation works similar to how it is used
in linear regression. Instead of using MSE, we use the number of 
misclassified observations to quantify test error. 

Cross validation is a method that checks how well the model fits test data.
It is used specifically when there is not a data set that is specifically used
as test data. Cross Validation allows us to use portions of the training data
as test data instead. 

For this project, we need to test and build each method with the same 
training and test data sets. We will use k-fold cross validation with a
k of 10. The code used to split the data can be found in the following section.

```{r k-fold cv} 
#set the seed so we get the same groups each time
set.seed(1)

#this section of code uses the r package caret to create 
#10 folds of the numbers from 1 to 1599. So if you do not have
#the caret package, you will need to install it. 

flds <- createFolds(c(1:1599), k = 10, list = TRUE, returnTrain = FALSE)
names(flds)[1] <- "train"

#Once I created the folds of the numbers between 1 and 1599, I used these to 
#pick the corresponding columns of the data frame. These can be used for 
#all the following methods to calculate test error using the k-fold 
#method of cross validation


fold1 <- winequality.red[flds$train,]
fold2 <- winequality.red[flds$Fold02,]
fold3 <- winequality.red[flds$Fold03,]
fold4 <- winequality.red[flds$Fold04,]
fold5 <- winequality.red[flds$Fold05,]
fold6 <- winequality.red[flds$Fold06,]
fold7 <- winequality.red[flds$Fold07,]
fold8 <- winequality.red[flds$Fold08,]
fold9 <- winequality.red[flds$Fold09,]
fold10 <- winequality.red[flds$Fold10,]
folds = list(fold1,fold2,fold3,fold4,fold5,fold6,fold7,fold8,fold9,fold10)

```


# - Madison
In this section, we will use the Quadratic Discriminant Analysis 
classification method. We will be doing it with the resampling method
of k-fold cross validation. Using a k = 10, we will fit the method 
10 different times in order to get a test error. 


```{r QDA}
#We need to initialize the structures that we will use to store things
testerror = NULL
training = NULL
i = 1

#We need to use a loop to do the QDA analysis 10 times. 
while(i <11){
  #this line selects the test fold
  testdata = folds[[i]]
  #this line puts the rest of the folds in a list of lists
  traindata = folds[-i]
  #this line changes the list of lists into a data frame
  training = dplyr::bind_rows(traindata)
  #this line fits the qda method to the data. Everything but the original 
  #quality are used as responses. We are predicting the quality based on 
  #the 0 and 1 method. It is also only fit on the training observations
  qda.fit = qda(Quality~. -quality, data=training)
  #This line uses the built in predict function to figure out what
  #our model would predict for the test data.
  qda.predict = predict(qda.fit, testdata)$class
  #this line calculates the average misclassification rate of each iteration
  testerror[i]= mean(qda.predict != testdata$Quality)
  #this is the iterator so we can move through the loop
  i = i +1
}

#this averages the test errors from the loop to find the overall test error
kfoldtesterror = mean(testerror)
print("Average error of the QDA method using 10-fold cross validation is")
format(kfoldtesterror)

```


#LDA - Chung Ho



```{r LDA}
split <- data.frame(flds[1])
split <- as.vector(split[,1])
wine.test <- wine[split,]
wine.train <- wine[-split,]

# LDA using train data
wine.model <- lda(Quality~., data = wine.train)
wine.model
lda.pred <- predict(wine.model, wine.test)
test.err.lda <- mean(lda.pred$class!=wine.test$Quality)

plot(wine.model)

# Prediction using LDA model
wine.pred <- wine.model %>% predict(wine.test)
mean( wine.pred$class == wine.test$Quality )
```

#KNN - Mike



```{r KNN}
#Use the same trainset from above in order to maintain consistency.
#Split the data into train and test to allow for kNN approach.
#Perform kNN model with k=39 because 39 is sqrt of 1599 (typical approach to use k=sqrt(n)). 
errors = NULL
for (i in 1:10) {
  split <- data.frame(flds[i])
  split <- as.vector(split[,1])
  test <- wine[split,]
  train <- wine[-split,]
  wineTrain_Target <- train[,12]
  knn.pred1 <- knn(train,test,wineTrain_Target,k=39)
  #Compute test errors for all folds.
  errors[i] <- mean(knn.pred1 != test$Quality)
}

#Create a table to show actual vs predicted
table(knn.pred1,test$Quality)
#Compute mean test error.
knn.test_error <- mean(errors)

#Perform kNN again, with k=5 this time.
errors2 = NULL
for (i in 1:10) {
  split <- data.frame(flds[i])
  split <- as.vector(split[,1])
  test <- wine[split,]
  train <- wine[-split,]
  wineTrain_Target <- train[,12]
  knn.pred2 <- knn(train,test,wineTrain_Target,k=5)
  #Compute errors across all folds.
  errors2[i] <- mean(knn.pred2 != test$Quality)
}

#Create a table to show actual vs predicted
table(knn.pred2,test$Quality)
#Compute mean test error.
knn.test_error2 <- mean(errors2)

#Perform kNN with k=10
errors3 = NULL
for (i in 1:10) {
  split <- data.frame(flds[i])
  split <- as.vector(split[,1])
  test <- wine[split,]
  train <- wine[-split,]
  wineTrain_Target <- train[,12]
  knn.pred3 <- knn(train,test,wineTrain_Target,k=10)
  #Compute errors across folds.
  errors3[i] <- mean(knn.pred3 != test$Quality)
}

#Create a table to show actual vs predicted
table(knn.pred3,test$Quality)
#Compute mean test error.
knn.test_error3 <- mean(errors3)

#Plot the errors of each of the kNN models with different k values
plot(errors,col="blue")
lines(errors,col="blue")
points(errors2,col="red")
lines(errors2,col="red")
points(errors3,col="green")
lines(errors3,col="green")
legend("topright",legend=c("k=39","k=5","k=10"),lty=c(1,1),col=c("blue","red","green"))
```



#Random Forest - Shao-Wei

First of all, we randomly select one of testing and training set to check the scale of the forest. It shows that the curve is gradually being stable after 200 trees from the below plot. Thus, the number of trees is 200 for later steps.

```{r}
  split<-data.frame(flds[1])
  split<-as.vector(split[,1])
  test<-wine[split,]
  y.test<-test[,12]
  x.test<- test[,1:11]
  train<-wine[-split,]
  RF<-randomForest(formula = Quality ~ ., data = train, xtest = x.test , ytest = y.test , ntree = 1000, mtry=round(sqrt(ncol(x.test))))
  plot(RF)
```

The number of m in each split is an important part to get influence to our test error. In order to reduce test error, we build the random forest by different m in each testing and training set, so there are totally 110 forests and 10 of each m, and then we compute the average error of each m.
Each line is the test error from different testing and training set, and the dashed line is their average. Although the graph indicate that m=6 is the better parameter, each tree could be more correlated as larger m is. Thus, m=4 which is close to the square root of the predictors 13 is our decision.

```{r Random Forest}
same.data.rf.error<-data.frame(sample=rep(1:10,each=11),m=rep(1:11,times=10),error=NA)
for (i in 1:10) {
  split<-data.frame(flds[i])
  split<-as.vector(split[,1])
  test<-wine[split,]
  y.test<-test[,12]
  x.test<- test[,1:11]
  train<-wine[-split,]
  for (m in 1:11) {
      RF<-randomForest(formula = Quality ~ ., data = train, xtest = x.test , ytest = y.test , ntree = 200, mtry=m)
      same.data.rf.error[11*(i-1)+m,3]<-RF$test$err.rate[200,1]
  }
}

#mean comparisonin in different m

ave.rf.error<-data.frame(sample=rep("Average"),m=c(1:11),error=rep(NA,11))
for (i in 1:11) {
  ave.rf.error[i,3]<-mean(same.data.rf.error[same.data.rf.error$m==i,3])
}
rf.error<-rbind(same.data.rf.error,ave.rf.error)
rf.error$sample<-as.factor(rf.error$sample)
rf.error<-cbind(rf.error,average=c(rep(0,110),rep(1,11)))
ggplot(rf.error,aes(x=m , y = error,color=sample,linetype=as.factor(average))) +
  geom_line() + guides(linetype=FALSE) + theme_bw()

```

Now, we build the forests again with m=4 and ntree=200 for each subsets, and compute the accuracy ((True Positive+True Negative)/Total) and the average accuracy. Moreover, the accuracy might be increased by removing some strongly uncorrelated predictors, and by checking the importance, we notice that `PH` and `free.sulfur.dioxide` are relatively less importance. Hence, we try to build the model without them and compute their accuracy. From the result, the model with all predictors is $0.9155464$; The model without `pH` is $0.9205621$;The model without `pH, free.sulfur.dioxide` is $0.9193042$; and The model without `free.sulfur.dioxide` is $0.9180503$. Thus, the forest without `pH` can slighly increase the accuracy and it's our final model.

```{r}
###RF with m=4###
accu<-matrix(NA,10,1)
accu1<-matrix(NA,10,1)
accu2<-matrix(NA,10,1)
accu3<-matrix(NA,10,1)
par(mfrow=c(2,5))
for (i in 1:10) {
  split<-data.frame(flds[i])
  split<-as.vector(split[,1])
  test<-wine[split,]
  y.test<-test[,12]
  x.test<- test[,1:11]
  train<-wine[-split,]
  RF<-randomForest(formula = Quality ~ ., data = train, xtest = x.test , ytest = y.test , ntree = 200, mtry=4)
  accu[i,]<-sum(test[,12]==RF$test$predicted)/length(test[,12]) #overall accuracy
  importance(RF)
  varImpPlot(RF, sort = TRUE)
}
mean(accu)
for (i in 1:10) {
  split<-data.frame(flds[i])
  split<-as.vector(split[,1])
  test<-wine[split,]
  y.test<-test[,12]
  x.test<- test[,-c(9,12)]
  train<-wine[-split,]
  RF<-randomForest(formula = Quality ~ .-pH, data = train, xtest = x.test , ytest = y.test , ntree = 200, mtry=3)
  accu1[i,]<-sum(test[,12]==RF$test$predicted)/length(test[,12]) #overall accuracy
}
mean(accu1)
for (i in 1:10) {
  split<-data.frame(flds[i])
  split<-as.vector(split[,1])
  test<-wine[split,]
  y.test<-test[,12]
  x.test<- test[,-c(6,9,12)]
  train<-wine[-split,]
  RF<-randomForest(formula = Quality ~ .-pH-free.sulfur.dioxide, data = train, xtest = x.test , ytest = y.test , ntree = 200, mtry=3)
  accu2[i,]<-sum(test[,12]==RF$test$predicted)/length(test[,12]) #overall accuracy
}
mean(accu2)
for (i in 1:10) {
  split<-data.frame(flds[i])
  split<-as.vector(split[,1])
  test<-wine[split,]
  y.test<-test[,12]
  x.test<- test[,-c(6,12)]
  train<-wine[-split,]
  RF<-randomForest(formula = Quality ~ .-free.sulfur.dioxide, data = train, xtest = x.test , ytest = y.test , ntree = 500, mtry=3)
  accu3[i,]<-sum(test[,12]==RF$test$predicted)/length(test[,12]) #overall accuracy
}
mean(accu3)
```






