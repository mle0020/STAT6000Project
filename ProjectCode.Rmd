---
title: "Stat6000Project"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries}
library(ggplot2)
library(caret)
library(tidyverse)
library(ISLR)
```


#Get Data Ready

First we need to read in our data set

```{r data set}
#below is one way to read in the data
#winequality.red <- read.csv("~/GitHub/STAT6000Project/winequality-red.csv", sep=";")

#This is another way to read in the data and the one that 
#we will use for the project
winequality.red<-read.csv("winequality-red.csv",sep = ";")
winequality.red
```

Now we need to make sure there are no empty values or missing values


```{r fix data}
summary(complete.cases(winequality.red))
```
Doing this check shows us that we do not have any na values in our data 
set.
Now lets do some summary statistics and plots of the response 
value of quality

```{r data check}
summary(winequality.red)
#box plot of the data
ggplot(data=winequality.red, mapping = aes(x=quality))+
  geom_boxplot()
#bar graph of the data
ggplot(data = winequality.red) + 
  geom_bar(mapping = aes(x = quality))+
  geom_vline(xintercept=mean(winequality.red$quality), color="purple")+
  geom_vline(xintercept=median(winequality.red$quality), color="cyan")

#Now we want to make a column we could potentially use for logistic regression
logstuff<-rep("No",dim(winequality.red)[1])
logstuff[winequality.red$quality>6.5]<-"Yes"

#this adds the column to our data set
winequality.red <- winequality.red %>%
  mutate(
    GoodWine = logstuff
  )
    

```

#Cross Validation for Classification
For classification problems, cross validation works similar to how it is used
in linear regression. Instead of using MSE, we use the number of 
misclassified observations to quantify test error. 

Cross validation is a method that checks how well the model fits test data.
It is used specifically when there is not a data set that is specifically used
as test data. Cross Validation allows us to use portions of the training data
as test data instead. 

For this project, we need to test and build each method with the same 
training and test data sets. We will use k-fold cross validation with a
k of 10. The code used to split the data can be found in the following section.

```{r k-fold cv} 
#set the seed so we get the same groups each time
set.seed(1)

#this section of code uses the r package caret to create 
#10 folds of the numbers from 1 to 1599. So if you do not have
#the caret package, you will need to install it. 

flds <- createFolds(c(1:1599), k = 10, list = TRUE, returnTrain = FALSE)
names(flds)[1] <- "train"

#Once I created the folds of the numbers between 1 and 1599, I used these to 
#pick the corresponding columns of the data frame. These can be used for 
#all the following methods to calculate test error using the k-fold 
#method of cross validation

trainset <- winequality.red[flds$train,]
fold2 <- winequality.red[flds$Fold02,]
fold3 <- winequality.red[flds$Fold03,]
fold4 <- winequality.red[flds$Fold04,]
fold5 <- winequality.red[flds$Fold05,]
fold6 <- winequality.red[flds$Fold06,]
fold7 <- winequality.red[flds$Fold07,]
fold8 <- winequality.red[flds$Fold08,]
fold9 <- winequality.red[flds$Fold09,]
fold10 <- winequality.red[flds$Fold10,]

```


# - Madison


```{r QDA}


```


#LDA - Chung Ho



```{r LDA}

```

#KNN - Mike



```{r KNN}

```



#Random Forest - Shao-Wei

```{r Random Forest}

```






