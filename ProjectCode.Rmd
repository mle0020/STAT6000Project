---
title: "Stat6000 Project"
output:
  html_document:
    highlight: tango
    theme: spacelab
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries,message=FALSE,warning=FALSE}
library(ggplot2)
library(caret)
library(tidyverse)
library(ISLR)
library(randomForest)
```


#Get Data Ready

First we need to read in our data set

```{r data set}
#below is one way to read in the data
#winequality.red <- read.csv("~/GitHub/STAT6000Project/winequality-red.csv", sep=";")

#This is another way to read in the data and the one that 
#we will use for the project
winequality.red<-read.csv("winequality-red.csv",sep = ";")
head(winequality.red)
```

Now we need to make sure there are no empty values or missing values


```{r fix data}
summary(complete.cases(winequality.red))
```

Doing this check shows us that we do not have any missing values in our data 
set.
Now lets do some summary statistics and plots of the response 
value of quality.

```{r data check}
summary(winequality.red)

#bar graph of the data
ggplot(data = winequality.red) + 
  geom_bar(mapping = aes(x = quality))+
  geom_vline(xintercept=mean(winequality.red$quality), color="purple")+
  geom_vline(xintercept=median(winequality.red$quality), color="cyan")+theme_bw()
```

From the data page, it points out that the wine is good if its quality is larger than 6.5, then we create a new factor variable which will be used for our model response, if `quality` is bigger than 6.5, it is equal to 1, otherwise, it is 0.
Moreover, we notice that the number of "bad" red wine is much more than the good ones, and it might influence our experiment.

```{r}
winequality.red$Quality <- rep(0,dim(winequality.red)[1])
winequality.red$Quality[winequality.red$quality>6.5] <- "1"
winequality.red$Quality <- as.factor(winequality.red$Quality)
wine<-winequality.red[,c(1:11,13)] ###dataset for below steps###
head(wine)
table(winequality.red$Quality)
```

Also, we will create a boxplot of each predictor and check the difference between 
good quality wine (Yes)and bad quality wine (No) for each variable. 
Except for `residual.sugar`, `chlorides`, `free.sulfur.dioxide`, and `PH`, 
there are obvious differences between "good" and "bad" wines. 
In addition, the box-plots show that there are a lot of outliers 
in different predictors, and it could be the reason for misclassification.

```{r}

#Now we want to make a column we could potentially use for logistic regression
logstuff<-rep("No",dim(winequality.red)[1])
logstuff[winequality.red$quality>6.5]<-"Yes"

#this adds the column to our data set
winequality.red <- winequality.red %>%
  mutate(
    GoodWine = logstuff
  )
winequality.red$GoodWine <- as.factor(winequality.red$GoodWine)
  
par(mfrow=c(2,3))
for (v in 1:11) {
  boxplot(winequality.red[,v]~GoodWine,data = winequality.red,
          ylab = colnames(winequality.red)[v] )
}
```

#Cross Validation for Classification
For classification problems, cross validation works similar to how it is used
in linear regression. Instead of using MSE, we use the number of 
misclassified observations to quantify test error. 

Cross validation is a method that checks how well the model fits test data.
It is used specifically when there is not a data set that is specifically used
as test data. Cross Validation allows us to use portions of the training data
as test data instead. 

For this project, we need to test and build each method with the same 
training and test data sets. We will use k-fold cross validation with a
k of 10. The code used to split the data can be found in the following section.

```{r k-fold cv} 
#set the seed so we get the same groups each time
set.seed(1)

#this section of code uses the r package caret to create 
#10 folds of the numbers from 1 to 1599. So if you do not have
#the caret package, you will need to install it. 

flds <- createFolds(c(1:1599), k = 10, list = TRUE, returnTrain = FALSE)
names(flds)[1] <- "train"

#Once I created the folds of the numbers between 1 and 1599, I used these to 
#pick the corresponding columns of the data frame. These can be used for 
#all the following methods to calculate test error using the k-fold 
#method of cross validation

fold1 <- winequality.red[flds$train,]
fold2 <- winequality.red[flds$Fold02,]
fold3 <- winequality.red[flds$Fold03,]
fold4 <- winequality.red[flds$Fold04,]
fold5 <- winequality.red[flds$Fold05,]
fold6 <- winequality.red[flds$Fold06,]
fold7 <- winequality.red[flds$Fold07,]
fold8 <- winequality.red[flds$Fold08,]
fold9 <- winequality.red[flds$Fold09,]
fold10 <- winequality.red[flds$Fold10,]

```


# - Madison


```{r QDA}


```


#LDA - Chung Ho



```{r LDA}

```

#KNN - Mike



```{r KNN}
#Use the same trainset from above in order to maintain consistency.
#Split the data into train and test to allow for kNN approach.
train <- winequality.red[fold1,]
test <- winequality.red[-fold1,]

#Standardize Data
process <- preProcess(train, method=c("center","scale"))
train_s <- predict(process,train)
test_s <- predict(process,test)

#Data Processing
wineTrain <- train_s[,-12]
wineTest <- test_s[,-12]
trainLabel <- train_s[,12,drop=TRUE]
testLabel <- test_s[,12,drop=TRUE]

#Perform the kNN model
knn.pred <- knn(wineTrain,wineTest,trainLabel,k=39)

#Evaluate Model
confusionMatrix(testLabel,knn.pred)
```



#Random Forest - Shao-Wei

First of all, we randomly select one of testing and training set to check the scale of the forest. It shows that the curve is gradually being stable after 200 trees from the below plot. Thus, the number of trees is 200 for later steps.

```{r}
  split<-data.frame(flds[1])
  split<-as.vector(split[,1])
  test<-wine[split,]
  y.test<-test[,12]
  x.test<- test[,1:11]
  train<-wine[-split,]
  RF<-randomForest(formula = Quality ~ ., data = train, xtest = x.test , ytest = y.test , ntree = 1000, mtry=round(sqrt(ncol(x.test))))
  plot(RF)
```

The number of m in each split is an important part to get influence to our test error. In order to reduce test error, we build the random forest by different m in each testing and training set, so there are totally 110 forests and 10 of each m, and then we compute the average error of each m.
Each line is the test error from different testing and training set, and the dashed line is their average. Although the graph indicate that m=6 is the better parameter, each tree could be more correlated as larger m is. Thus, m=4 which is close to the square root of the predictors 13 is our decision.

```{r Random Forest}
same.data.rf.error<-data.frame(sample=rep(1:10,each=11),m=rep(1:11,times=10),error=NA)
for (i in 1:10) {
  split<-data.frame(flds[i])
  split<-as.vector(split[,1])
  test<-wine[split,]
  y.test<-test[,12]
  x.test<- test[,1:11]
  train<-wine[-split,]
  for (m in 1:11) {
      RF<-randomForest(formula = Quality ~ ., data = train, xtest = x.test , ytest = y.test , ntree = 200, mtry=m)
      same.data.rf.error[11*(i-1)+m,3]<-RF$test$err.rate[200,1]
  }
}

#mean comparisonin in different m

ave.rf.error<-data.frame(sample=rep("Average"),m=c(1:11),error=rep(NA,11))
for (i in 1:11) {
  ave.rf.error[i,3]<-mean(same.data.rf.error[same.data.rf.error$m==i,3])
}
rf.error<-rbind(same.data.rf.error,ave.rf.error)
rf.error$sample<-as.factor(rf.error$sample)
rf.error<-cbind(rf.error,average=c(rep(0,110),rep(1,11)))
ggplot(rf.error,aes(x=m , y = error,color=sample,linetype=as.factor(average))) +
  geom_line() + guides(linetype=FALSE) + theme_bw()

```

Now, we build the forests again with m=4 and ntree=200 for each subsets, and compute the accuracy ((True Positive+True Negative)/Total) and the average accuracy. Moreover, the accuracy might be increased by removing some strongly uncorrelated predictors, and by checking the importance, we notice that `PH` and `free.sulfur.dioxide` are relatively less importance. Hence, we try to build the model without them and compute their accuracy. From the result, the model with all predictors is $0.9155464$; The model without `pH` is $0.9205621$;The model without `pH, free.sulfur.dioxide` is $0.9193042$; and The model without `free.sulfur.dioxide` is $0.9180503$. Thus, the forest without `pH` can slighly increase the accuracy and it's our final model.

```{r}
###RF with m=4###
accu<-matrix(NA,10,1)
accu1<-matrix(NA,10,1)
accu2<-matrix(NA,10,1)
accu3<-matrix(NA,10,1)
par(mfrow=c(2,5))
for (i in 1:10) {
  split<-data.frame(flds[i])
  split<-as.vector(split[,1])
  test<-wine[split,]
  y.test<-test[,12]
  x.test<- test[,1:11]
  train<-wine[-split,]
  RF<-randomForest(formula = Quality ~ ., data = train, xtest = x.test , ytest = y.test , ntree = 200, mtry=4)
  accu[i,]<-sum(test[,12]==RF$test$predicted)/length(test[,12]) #overall accuracy
  importance(RF)
  varImpPlot(RF, sort = TRUE)
}
mean(accu)
for (i in 1:10) {
  split<-data.frame(flds[i])
  split<-as.vector(split[,1])
  test<-wine[split,]
  y.test<-test[,12]
  x.test<- test[,-c(9,12)]
  train<-wine[-split,]
  RF<-randomForest(formula = Quality ~ .-pH, data = train, xtest = x.test , ytest = y.test , ntree = 200, mtry=3)
  accu1[i,]<-sum(test[,12]==RF$test$predicted)/length(test[,12]) #overall accuracy
}
mean(accu1)
for (i in 1:10) {
  split<-data.frame(flds[i])
  split<-as.vector(split[,1])
  test<-wine[split,]
  y.test<-test[,12]
  x.test<- test[,-c(6,9,12)]
  train<-wine[-split,]
  RF<-randomForest(formula = Quality ~ .-pH-free.sulfur.dioxide, data = train, xtest = x.test , ytest = y.test , ntree = 200, mtry=3)
  accu2[i,]<-sum(test[,12]==RF$test$predicted)/length(test[,12]) #overall accuracy
}
mean(accu2)
for (i in 1:10) {
  split<-data.frame(flds[i])
  split<-as.vector(split[,1])
  test<-wine[split,]
  y.test<-test[,12]
  x.test<- test[,-c(6,12)]
  train<-wine[-split,]
  RF<-randomForest(formula = Quality ~ .-free.sulfur.dioxide, data = train, xtest = x.test , ytest = y.test , ntree = 500, mtry=3)
  accu3[i,]<-sum(test[,12]==RF$test$predicted)/length(test[,12]) #overall accuracy
}
mean(accu3)
```






